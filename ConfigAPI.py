import requests # Fazer conex√µes
import pandas as pd # Tratamento de dados
import math  # Biblioteca para opera√ß√µes matem√°ticas (arredondar n√∫mero de p√°ginas)
from urllib.parse import urlparse #Manipular a URL
import streamlit as st # Site


# ===========================
# CONFIGURA√á√ÉO DA URL
# ===========================


def config_extrator(url, quantidade):
    parsed = urlparse(url)  # Pega a URL e atribui a fun√ß√£o parse
    path = parsed.path  # aqui ele vai mostrar tudo que vier depois do dominio, ou seja o caminho todo
    parts = path.split('/')  # aqui voc√™ esta dizendo que a barra √© o que divide cada informa√ß√£o da url

    term = parts[3]
    coords = parts[4]

    term = term.replace('+', '_')  # Transforma o + em _

    # ===========================
    # CONFIGURA√á√ÉO DA API
    # ===========================

    API_KEY = "9d3659aacc443aca0a7475f087d05f36b8e42a3d61221a8c232cb77c7dc1a435"  # üî• Coloque sua chave da SerpApi aqui

    # ===========================
    # PAR√ÇMETROS DA BUSCA FIXOS
    # ===========================

    base_url = "https://serpapi.com/search"
    base_params = {
        "engine": "google_maps",  # Motor de busca: Google Maps
        "q": term,  # Palavra-chave da busca
        "ll": coords,  # Coordenadas (latitude, longitude, zoom)
        "type": "search",  # Tipo de busca (search padr√£o)
        "api_key": API_KEY  # Sua chave da API
    }

    # ===========================
    # INPUT DO USU√ÅRIO
    # ===========================

    # Cada p√°gina retorna at√© 20 resultados, ent√£o calculamos quantas p√°ginas s√£o necess√°rias
    # Usamos math.ceil para arredondar pra cima, garantindo que tenha resultados suficientes
    numero_paginas = math.ceil(quantidade / 20)

    print(f"\nüîÑ Buscando {quantidade} resultados em {numero_paginas} p√°ginas...\n")


    # ===========================
    # FUN√á√ÉO PARA EXTRAIR A P√ÅGINA BASEADA NO PARAMETRO START, SE FOR 20 √â A PRIMEIRA SE FOR 40 A SEGUNDA ETC
    # ===========================

    def extrair_dados(start):
        # Copia os par√¢metros base e adiciona o 'start' que controla a pagina√ß√£o
        params = base_params.copy()
        params["start"] = start
        # Faz a requisi√ß√£o para a API
        response = requests.get(base_url, params=params)
        if response.status_code == 200:
            return response.json()  # Retorna a resposta em JSON
        else:
            # Se der erro, mostra o erro e retorna None
            print(f"‚ùå Erro na requisi√ß√£o: {response.status_code}")
            print(response.text)
            return None

    # ===========================
    # LOOP PARA EXTRAIR DADOS
    # ===========================

    dados_extraidos = []  # Lista onde os dados v√£o ser armazenados

    for pagina in range(numero_paginas):
        # O par√¢metro 'start' √© sempre pagina * 20 ‚Üí 0, 20, 40, 60...
        start = pagina * 20
        resultado = extrair_dados(start)

        if resultado and "local_results" in resultado:  # Aqui ele verifica se teve extra√ß√£o dos dados e tem o local results dentro de resultado
            for item in resultado["local_results"]:  # Aqui ele percorre a lista dos resultados dos locais
                dados = {  # Aqui ele ta extraindo os dados que vem em JSON para colocar em um dicion√°rio
                    "Nome": item.get("title"),
                    "Endere√ßo": item.get("address"),
                    "Telefone": item.get("phone"),
                    "Nota": item.get("rating"),
                    "N¬∫ Avalia√ß√µes": item.get("reviews"),
                    "Website": item.get("website"),
                    "Hor√°rio": item.get("hours"),
                }
                dados_extraidos.append(dados)  # Adiciona o dicion√°rio em uma lista

            print(f"‚úÖ P√°gina {pagina + 1} coletada com sucesso!")
        else:
            print(f"‚ö†Ô∏è Nenhum resultado encontrado na p√°gina {pagina + 1}")

    # ===========================
    # TRATAR SE EXTRAIU MAIS QUE O PEDIDO
    # ===========================

    # Corta a lista para ter exatamente a quantidade que o usu√°rio pediu (√†s vezes vem mais na √∫ltima p√°gina)
    dados_extraidos = dados_extraidos[:quantidade]

    # ===========================
    # ORGANIZANDO OS DADOS EM PLANILHA
    # ===========================

    # Pega os dados dentro da lista e organiza em tabela/planilha
    df = pd.DataFrame(dados_extraidos)

    # ===========================
    # MOSTRANDO OS DADOS NO SITE
    # ===========================

    st.markdown("### üëá Primeiros 3 resultados encontrados:")
    st.dataframe(df.head(3))  # Mostra s√≥ os 3 primeiros

    # Bot√£o de download
    st.download_button(
        label="üì• Baixar CSV",
        data=df.to_csv(index=False).encode("utf-8-sig"),
        file_name="resultados.csv",
        mime="text/csv"
    )




"""# ===========================
# SALVANDO EM CSV
# ===========================

df.to_csv(f"{term}.csv", index=False, encoding="utf-8-sig")
print(f"\nüìÅ Arquivo {term}.csv salvo com sucesso!")"""
